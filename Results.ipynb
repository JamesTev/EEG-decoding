{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d55aedd-a25e-44fe-8f8e-b585dfeaec40",
   "metadata": {},
   "source": [
    "## Results and Plotting\n",
    "A notebook to compile results and organise plots and other resources needed for reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e883f67b-0190-44d6-a7a5-562437ea9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set global plotting params here for consistency\n",
    "plt.rcParams['axes.titlesize'] = 20\n",
    "plt.rcParams['axes.labelsize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 14\n",
    "plt.rcParams['ytick.labelsize'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a5be5b9a-60fe-4bf5-9b50-05f740ab15dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_freqs = [7,10,12] # stim freqs used\n",
    "fs = 64 # sampling freq\n",
    "Ns = 256 # number of sample points to consider\n",
    "Nh = 1 # number of harmonics for CCA-based algos\n",
    "\n",
    "index_pos = dict(zip([\"Nc\", \"Ns\", \"Nt\"], range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c367f892-254c-47c4-8704-0932e9b53011",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load data from json log files and arrange by frequency. The compiled data is stored in the dictionary `data` whose keys are the stimulus frequencies used and whose values are the data tensors corresponding to trials at those frequencies. Data tensors will be arranged like `Nc x Ns x Nt` (channels x samples x trials). \n",
    "\n",
    "Note that in this project, we only effectively had one channel. Also, all Nt trials would be independent recordings at the same stimulus frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "79fbe544-4b0d-41d1-bf11-6879fd38f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eeg_lib.utils import read_json\n",
    "import json\n",
    "\n",
    "tests = {7: [\"test-7hz-pos2\"], \n",
    "         10: [\"test-10hz-pos2\"], \n",
    "         12: [\"test-12hz-pos2\"]\n",
    "        }\n",
    "\n",
    "all_data = read_json('eeg_lib/log_data.json')\n",
    "data = {}\n",
    "\n",
    "for f, test_set in tests.items():\n",
    "    data[f] = []\n",
    "    \n",
    "    for test in test_set:\n",
    "        values = all_data[test]\n",
    "        proc_data = np.array([json.loads(values[i]) for i in range(len(values))])\n",
    "        data[f].append(proc_data[1:, :Ns].reshape((1, Ns, -1))) # exclude first trial\n",
    "        \n",
    "# del all_data    \n",
    "\n",
    "for f, proc_data in data.items():\n",
    "    if len(proc_data) <= 1:\n",
    "        data[f] = proc_data[0]\n",
    "    else:\n",
    "        data[f] = np.concatenate([*proc_data], axis=-1) # merge data from across trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a153db0-2a17-4364-a0f6-6e0d69f286e2",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "Run various decoding algos on gathered data and store results for comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2455b-006c-4ac8-9fa9-de88015a77a7",
   "metadata": {},
   "source": [
    "### CCA\n",
    "Vanilla CCA with no historical training data used across evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ed17014c-b4df-415d-b72f-c824a621fca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>y</th>\n",
       "      <th>y_hat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trial</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f7_1</th>\n",
       "      <td>0.147529</td>\n",
       "      <td>0.071546</td>\n",
       "      <td>0.018522</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7_2</th>\n",
       "      <td>0.119633</td>\n",
       "      <td>0.118277</td>\n",
       "      <td>0.074220</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7_3</th>\n",
       "      <td>0.191390</td>\n",
       "      <td>0.108617</td>\n",
       "      <td>0.030403</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7_4</th>\n",
       "      <td>0.171472</td>\n",
       "      <td>0.092428</td>\n",
       "      <td>0.082114</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f7_5</th>\n",
       "      <td>0.087934</td>\n",
       "      <td>0.091173</td>\n",
       "      <td>0.105607</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10_1</th>\n",
       "      <td>0.202168</td>\n",
       "      <td>0.020386</td>\n",
       "      <td>0.045348</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10_2</th>\n",
       "      <td>0.246756</td>\n",
       "      <td>0.051406</td>\n",
       "      <td>0.112536</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10_3</th>\n",
       "      <td>0.189889</td>\n",
       "      <td>0.072409</td>\n",
       "      <td>0.182558</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10_4</th>\n",
       "      <td>0.206812</td>\n",
       "      <td>0.064271</td>\n",
       "      <td>0.081093</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10_5</th>\n",
       "      <td>0.215907</td>\n",
       "      <td>0.021243</td>\n",
       "      <td>0.065611</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f10_6</th>\n",
       "      <td>0.230957</td>\n",
       "      <td>0.126660</td>\n",
       "      <td>0.089375</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_1</th>\n",
       "      <td>0.060042</td>\n",
       "      <td>0.097416</td>\n",
       "      <td>0.042605</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_2</th>\n",
       "      <td>0.026629</td>\n",
       "      <td>0.112530</td>\n",
       "      <td>0.042660</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_3</th>\n",
       "      <td>0.035745</td>\n",
       "      <td>0.085280</td>\n",
       "      <td>0.044260</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_4</th>\n",
       "      <td>0.074395</td>\n",
       "      <td>0.113461</td>\n",
       "      <td>0.043385</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_5</th>\n",
       "      <td>0.014238</td>\n",
       "      <td>0.055778</td>\n",
       "      <td>0.042370</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_6</th>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.044958</td>\n",
       "      <td>0.040701</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f12_7</th>\n",
       "      <td>0.097154</td>\n",
       "      <td>0.038268</td>\n",
       "      <td>0.057530</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              7        10        12   y  y_hat\n",
       "trial                                         \n",
       "f7_1   0.147529  0.071546  0.018522   7      7\n",
       "f7_2   0.119633  0.118277  0.074220   7      7\n",
       "f7_3   0.191390  0.108617  0.030403   7      7\n",
       "f7_4   0.171472  0.092428  0.082114   7      7\n",
       "f7_5   0.087934  0.091173  0.105607   7     12\n",
       "f10_1  0.202168  0.020386  0.045348  10      7\n",
       "f10_2  0.246756  0.051406  0.112536  10      7\n",
       "f10_3  0.189889  0.072409  0.182558  10      7\n",
       "f10_4  0.206812  0.064271  0.081093  10      7\n",
       "f10_5  0.215907  0.021243  0.065611  10      7\n",
       "f10_6  0.230957  0.126660  0.089375  10      7\n",
       "f12_1  0.060042  0.097416  0.042605  12     10\n",
       "f12_2  0.026629  0.112530  0.042660  12     10\n",
       "f12_3  0.035745  0.085280  0.044260  12     10\n",
       "f12_4  0.074395  0.113461  0.043385  12     10\n",
       "f12_5  0.014238  0.055778  0.042370  12     10\n",
       "f12_6  0.054800  0.044958  0.040701  12      7\n",
       "f12_7  0.097154  0.038268  0.057530  12      7"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from eeg_lib.cca import CCA\n",
    "\n",
    "cca = CCA(stim_freqs, fs, Nh=Nh)\n",
    "\n",
    "cca_results = {f:[] for f in stim_freqs}\n",
    "cca_agg_results = {f:{} for f in stim_freqs}\n",
    "\n",
    "for f in stim_freqs:\n",
    "    data_f = data[f]\n",
    "    for trial in range(1, data[f].shape[index_pos[\"Nt\"]]):\n",
    "        Xi = data_f[:, :, trial]\n",
    "        result = cca.compute_corr(Xi)\n",
    "        result = {k:np.round(v[0], 6) for k,v in result.items()}\n",
    "        \n",
    "        result['trial'] = f'f{f}_{trial}'\n",
    "        result['y'] = f\n",
    "        cca_results[f].append(result)\n",
    "        \n",
    "        # compute CCA result using data aggregated across trials\n",
    "        agg_result = cca.compute_corr(data_f.mean(axis=index_pos[\"Nt\"]))\n",
    "        agg_result = {k:np.round(v[0], 6) for k,v in agg_result.items()}\n",
    "        agg_result['y'] = f\n",
    "        cca_agg_results[f] = agg_result\n",
    "    \n",
    "cca_df = pd.concat([pd.DataFrame(result_set) for result_set in cca_results.values()]).set_index('trial')\n",
    "cca_df['y_hat'] = cca_df[stim_freqs].apply(lambda row: stim_freqs[np.argmax(row)], axis=1)\n",
    "\n",
    "cca_agg_df = pd.DataFrame(list(cca_agg_results.values()))\n",
    "cca_agg_df['y_hat'] = cca_agg_df[stim_freqs].apply(lambda row: stim_freqs[np.argmax(row)], axis=1)\n",
    "\n",
    "cca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e9446-9134-4c2e-905d-0b970d8117e3",
   "metadata": {},
   "source": [
    "## Template-based Algorithms \n",
    "This section explores decoding algos that, along with potentially the artificially-generated harmonic reference, include template data based on historical 'training' data. These include GCCA, MsetCCA, TRCA and others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f6d25150-fa26-4f61-8082-ddf8d676b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data tensor shape:  (3, 1, 256, 6)\n"
     ]
    }
   ],
   "source": [
    "min_trial_len = np.min([test_set.shape[-1] for test_set in data.values()])\n",
    "\n",
    "# Nf x Nc x Ns x Nt\n",
    "data_tensor = np.array([test_set[:,:,:min_trial_len] for test_set in data.values()])\n",
    "\n",
    "print(\"Data tensor shape: \", data_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "a9fba2b5-e3e3-49b4-89cb-f5a73e40609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "N_train = 3\n",
    "lpo = LeavePOut(p=N_train)\n",
    "\n",
    "n_trials = data_tensor.shape[-1]\n",
    "template_idxs = list(lpo.split(range(n_trials)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418c9f7-edd4-4a52-9c94-f9bf7982aaff",
   "metadata": {},
   "source": [
    "#### GCCA\n",
    "Generalised CCA aims to simultaneously maximise correlation between three sets of data: historical observations, measured signals in a new sample and the pre-constructed sinusoidal reference. As interpreted by the authors (Wong et al), the optimal spatial filters obtained through GCCA perform SSVEP signal denoising.\n",
    "\n",
    "#### MsetCCA\n",
    "MsetCCA is one extension of standard CCA that takes into account historical data instead of performing inference purely on new observations. Zhang et al propose that this is one of the reasons that standard CCA performs poorly on short time windows; it effectively over fits to localised dynamics. Furthermore, the authors suggest that exclusively using the pre-constructed sinusoidal reference set is not optimal since this artificial reference does not exclude other features from real EEG data. To circumvent this, MsetCCA seeks to optimise the reference signals used in the CCA algorithm by learning multiple linear transforms to maximise overall correlation between canonical variables over many sets of EEG data at each candidate frequency fk ∈ F. This optimisation effectively finds optimal joint spatial filters w1, . . . , wNt (over Nt trials) using only historical observations (‘training’ data). The authors claim that MsetCCA outperforms similar techniques, especially in cases with few channels and short time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c40a4c-1a89-4690-a71b-340872fd5704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    exps = np.exp(X)\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy(X, y):\n",
    "    \"\"\"\n",
    "    source: https://deepnotes.io/softmax-crossentropy\n",
    "    \n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = softmax(X)\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "af2390bc-cf32-4ea5-93e0-98ba262194f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCCA:  {7: 1.0, 10: 0.9833333333333332, 12: 0.85}\n",
      "MsetCCA:  {7: 0.9833333333333332, 10: 0.9833333333333332, 12: 0.9166666666666667}\n"
     ]
    }
   ],
   "source": [
    "from eeg_lib.cca import GCCA_SSVEP\n",
    "from eeg_lib.cca import MsetCCA_SSVEP\n",
    "\n",
    "def compute_gcca_msetcca_results(gcca, mset_cca, data_tensor, stim_freqs, template_idxs, ce_loss=True):\n",
    "    \n",
    "    gcca_results = {f:[] for f in stim_freqs}\n",
    "    mset_cca_results = {f:[] for f in stim_freqs}\n",
    "\n",
    "    for f_idx, f in enumerate(stim_freqs):\n",
    "        for split_idx, (test_idxs, train_idxs) in enumerate(template_idxs):\n",
    "            chi_train = data_tensor[:, :, :, train_idxs]\n",
    "\n",
    "            # train models on current train-test split\n",
    "            gcca.fit(chi_train)\n",
    "            mset_cca.fit(chi_train)\n",
    "\n",
    "            # extract test matrices from all test indices and compute result\n",
    "            for test_idx in test_idxs:\n",
    "                if test_idx in train_idxs:\n",
    "                    raise ValueError(\"Found intersection between train and test indices\")\n",
    "                    \n",
    "                # note: we must match the number of samples Ns in chi_train\n",
    "                X_test = data_tensor[f_idx, :, :, test_idx]\n",
    "                \n",
    "                _idx = f'f{f}_split{split_idx+1}_test{test_idx+1}'\n",
    "                test_meta = {'idx': _idx, 'y': f, 'test': test_idx, 'split': split_idx}\n",
    "                \n",
    "                # GCCA\n",
    "                result = {k: abs(np.round(v,4)) for k,v in gcca.classify(X_test).items()}\n",
    "                gcca_results[f].append({**result, **test_meta})\n",
    "\n",
    "                # MsetCCA\n",
    "                result = {k: abs(np.round(v,4)) for k,v in mset_cca.classify(X_test).items()}\n",
    "                mset_cca_results[f].append({**result, **test_meta})\n",
    "                \n",
    "    def _prep_results_df(results):\n",
    "        df = pd.concat([pd.DataFrame(result_set) for result_set in results.values()])\n",
    "        df['y_hat'] = df[stim_freqs].apply(lambda row: stim_freqs[np.argmax(row)], axis=1)\n",
    "        df = df.set_index(['y', 'split'])\n",
    "        \n",
    "        if ce_loss:\n",
    "            # compute cross entropy loss\n",
    "            for f_idx, f in enumerate(stim_freqs):\n",
    "                result = df.loc[f, stim_freqs].apply(lambda row: cross_entropy(row.values.reshape(1, -1), np.array([f_idx])), axis=1)\n",
    "                df.loc[(f, ), 'ce_loss'] = result.values\n",
    "                \n",
    "        df['correct'] = df.index.get_level_values(level=0) == df.y_hat\n",
    "\n",
    "        return df\n",
    "\n",
    "    gcca_df = _prep_results_df(gcca_results)\n",
    "    mset_df = _prep_results_df(mset_cca_results)\n",
    "    \n",
    "    return gcca_df, mset_df\n",
    "\n",
    "def decoding_acc(result_df):\n",
    "    acc = result_df['correct'].groupby(['y', 'split']).apply(lambda x: np.sum(x)/len(x))\n",
    "    acc_grouped = acc.groupby('y')\n",
    "    acc_av = acc_grouped.mean().to_dict()\n",
    "    acc_std = acc_grouped.std().to_dict()\n",
    "    return {'raw': acc, 'mean': acc_av, 'std': acc_std}\n",
    "\n",
    "gcca = GCCA_SSVEP(stim_freqs, fs, Nh=Nh)\n",
    "mset_cca = MsetCCA_SSVEP(stim_freqs)\n",
    "\n",
    "gcca_df, mset_cca_df = compute_gcca_msetcca_results(gcca, mset_cca, data_tensor, stim_freqs, template_idxs)\n",
    "gcca_acc = decoding_acc(gcca_df)\n",
    "mset_acc = decoding_acc(mset_cca_df)\n",
    "\n",
    "print(\"GCCA: \", gcca_acc['mean'])\n",
    "print(\"MsetCCA: \", mset_acc['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "3bc3f44c-db8f-4f62-b6ab-aee396a3d268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "      <th>12</th>\n",
       "      <th>idx</th>\n",
       "      <th>test</th>\n",
       "      <th>y_hat</th>\n",
       "      <th>ce_loss</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th>split</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">7</th>\n",
       "      <th>0</th>\n",
       "      <td>0.1106</td>\n",
       "      <td>0.0153</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>f7_split1_test4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1.033814</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.0106</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>f7_split1_test5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1.055462</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0540</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>f7_split1_test6</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1.066056</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2370</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>f7_split2_test3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.950971</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1757</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>f7_split2_test5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0.986886</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">12</th>\n",
       "      <th>18</th>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>f12_split19_test2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1.013977</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>f12_split19_test4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0.973267</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>f12_split20_test1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1.094004</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>f12_split20_test2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1.046450</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0168</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0943</td>\n",
       "      <td>f12_split20_test3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1.045998</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>180 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               7      10      12                idx  test  y_hat   ce_loss  \\\n",
       "y  split                                                                     \n",
       "7  0      0.1106  0.0153  0.0082    f7_split1_test4     3      7  1.033814   \n",
       "   0      0.0725  0.0106  0.0035    f7_split1_test5     4      7  1.055462   \n",
       "   0      0.0540  0.0089  0.0006    f7_split1_test6     5      7  1.066056   \n",
       "   1      0.2370  0.0005  0.0124    f7_split2_test3     2      7  0.950971   \n",
       "   1      0.1757  0.0048  0.0013    f7_split2_test5     4      7  0.986886   \n",
       "...          ...     ...     ...                ...   ...    ...       ...   \n",
       "12 18     0.0013  0.0003  0.1306  f12_split19_test2     1     12  1.013977   \n",
       "   18     0.0030  0.0053  0.1986  f12_split19_test4     3     12  0.973267   \n",
       "   19     0.0256  0.0004  0.0200  f12_split20_test1     0      7  1.094004   \n",
       "   19     0.0010  0.0004  0.0800  f12_split20_test2     1     12  1.046450   \n",
       "   19     0.0168  0.0118  0.0943  f12_split20_test3     2     12  1.045998   \n",
       "\n",
       "          correct  \n",
       "y  split           \n",
       "7  0         True  \n",
       "   0         True  \n",
       "   0         True  \n",
       "   1         True  \n",
       "   1         True  \n",
       "...           ...  \n",
       "12 18        True  \n",
       "   18        True  \n",
       "   19       False  \n",
       "   19        True  \n",
       "   19        True  \n",
       "\n",
       "[180 rows x 8 columns]"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1274dc21-9f6b-4843-8df8-b57a5f40eee1",
   "metadata": {},
   "source": [
    "#### Tests to explore\n",
    "Some ideas for interesting tests/factors to investigate. \n",
    "\n",
    "Test the effect of the following on decoding accuracy:\n",
    "1. number of training trials\n",
    "2. number of samples in each window (Ns)\n",
    "3. number of stimulus frequencies\n",
    "\n",
    "other miscellaneous tests:\n",
    "- generalisation performance on different set of data: both with pretraining from diff sets and without\n",
    "- average accuracy per stimulus frequency\n",
    "- average accuracy per stimulus square configuration (wide, narrow etc) * optional\n",
    "- some meausre of inter-trial consistency \n",
    "- some measure of similarity between estimated outputs: e.g. log loss that penalises similar outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfcb401-0895-42a9-8185-f3d7a1b203cc",
   "metadata": {},
   "source": [
    "#### 1. Acc vs number of training trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "83ffb49d-1c87-44f0-9adc-6b7134f3e02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = data_tensor.shape[-1]\n",
    "gcca_ntr_acc = []\n",
    "mset_ntr_acc = []\n",
    "\n",
    "for n_train in range(1, n_trials):\n",
    "    lpo = LeavePOut(p=n_train)\n",
    "    template_idxs = list(lpo.split(range(n_trials)))\n",
    "\n",
    "    data_tensor_tmp = data_tensor[:, :, :Ns, :]\n",
    "    gcca_df, mset_cca_df = compute_gcca_msetcca_results(gcca, mset_cca, data_tensor_tmp, stim_freqs, template_idxs)\n",
    "\n",
    "    _gcca_acc = decoding_acc(gcca_df)\n",
    "    _mset_acc = decoding_acc(mset_cca_df)\n",
    "    \n",
    "    # store these values for easy plotting\n",
    "    gcca_ntr_acc.append({**_gcca_acc['mean'], **{\"n_train\": n_train}})\n",
    "    mset_ntr_acc.append({**_mset_acc['mean'], **{\"n_train\": n_train}})\n",
    "    \n",
    "acc_ntr_gcca = pd.DataFrame(gcca_ntr_acc).set_index(\"n_train\")\n",
    "acc_ntr_mset = pd.DataFrame(mset_ntr_acc).set_index(\"n_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c770bd-2ce3-4c15-85d9-a97d35a7ef12",
   "metadata": {},
   "source": [
    "#### 2. Acc vs number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "7477a8cb-5a93-43e5-98e0-c15647e0e795",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_range = [64, 128, 192, 256]\n",
    "gcca_acc = []\n",
    "mset_acc = []\n",
    "\n",
    "for ns in ns_range:\n",
    "    data_tensor_tmp = data_tensor[:, :, :ns, :]\n",
    "    gcca_df, mset_cca_df = compute_gcca_msetcca_results(gcca, mset_cca, data_tensor_tmp, stim_freqs, template_idxs, ce_loss=False)\n",
    "    _gcca_acc = decoding_acc(gcca_df)\n",
    "    _mset_acc = decoding_acc(mset_cca_df)\n",
    "    \n",
    "    # store these values for easy plotting\n",
    "    gcca_acc.append({**_gcca_acc['mean'], **{\"Ns\": ns}})\n",
    "    mset_acc.append({**_mset_acc['mean'], **{\"Ns\": ns}})\n",
    "    \n",
    "acc_ns_gcca = pd.DataFrame(gcca_acc).set_index(\"Ns\")\n",
    "acc_ns_mset = pd.DataFrame(mset_acc).set_index(\"Ns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a0e99ace-bf7d-43e3-96e2-6c7be782f072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5, -0.30000000000000004, -0.1, 0.1, 0.30000000000000004, 0.5]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_x_offsets(n, width=0.2):\n",
    "    \"get x offsets of bar centres for grouped bar charts in matplotlib\"\n",
    "    if n%2 == 0: # even\n",
    "        sides = [width*x/2 for x in range(1, n+1, 2)]\n",
    "        return [-1*x for x in sides[::-1]] + sides\n",
    "    else: # odd\n",
    "        sides = [width*x/2 for x in range(3, n+1, 2)]\n",
    "        return [-1*x for x in sides[::-1]] + [0] + sides\n",
    "\n",
    "    \n",
    "def grouped_bar(x, Y):\n",
    "        \n",
    "    fig, ax = plt.subplots(1, figsize=(16, 6))\n",
    "    for y in Y:\n",
    "        plt.bar()\n",
    "    \n",
    "# plot bars\n",
    "plt.bar(x - 0.3, df_grouped['NA_Sales'], width = 0.2, color = '#1D2F6F')\n",
    "plt.bar(x - 0.1, df_grouped['EU_Sales'], width = 0.2, color = '#8390FA')\n",
    "plt.bar(x + 0.1, df_grouped['JP_Sales'], width = 0.2, color = '#6EAF46')\n",
    "plt.bar(x + 0.3, df_grouped['Other_Sales'], width = 0.2, color = '#FAC748')\n",
    "# remove spines\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "# x y details\n",
    "plt.ylabel('Millions of copies')\n",
    "plt.xticks(x, df_grouped.index)\n",
    "plt.xlim(-0.5, 31)\n",
    "# grid lines\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(color='gray', linestyle='dashed', alpha=0.2)\n",
    "# title and legend\n",
    "plt.title('Video Game Sales By Platform and Region', loc ='left')\n",
    "plt.legend(['NA', 'EU', 'JP', 'Others'], loc='upper left', ncol = 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca095750-65a5-4aa7-9986-135e6946b245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg_env",
   "language": "python",
   "name": "eeg_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
